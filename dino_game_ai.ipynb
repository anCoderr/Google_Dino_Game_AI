{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSS used from screen capture\n",
    "from mss import mss\n",
    "# Sending commands to game.\n",
    "import pydirectinput\n",
    "\n",
    "# Googles OCR package used to know when the game ends.\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# OpenCV allows us to do frame processing\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "# Environment components\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebGame(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.crop_width, self.crop_height = 80, 60\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,self.crop_height,self.crop_width), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "        with mss() as sct:\n",
    "            self.mon0 = sct.monitors[0]\n",
    "            self.mon1 = sct.monitors[1]\n",
    "            self.mon2 = sct.monitors[2]\n",
    "            print(\"Net Display:\", self.mon0)\n",
    "            print(\"Display 1:  \", self.mon1)\n",
    "            print(\"Display 2:  \", self.mon2)\n",
    "\n",
    "        # Extraction parameters for the game.\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top':self.mon2['top'] + 130, 'left':self.mon2['left'] + 100, 'width':400, 'height':300}\n",
    "        self.done_location = {'top':self.mon2['top'] + 210, 'left':self.mon2['left'] + 480, 'width':190, 'height':45}\n",
    "\n",
    "    def step(self, action):\n",
    "        # 0 = Jump, 1 = Duck, 2 = No Action\n",
    "        action_map = {0: 'space', 1: 'down', 2: 'no_op'}\n",
    "        if action != 2:\n",
    "            pydirectinput.press(action_map[action])\n",
    "        done, _ = self.get_done()\n",
    "        new_observation = self.get_observation()\n",
    "        reward = 1\n",
    "        info = {}\n",
    "        return new_observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        time.sleep(0.01)\n",
    "        pydirectinput.click(x=self.mon2['top'] + 200, y=self.mon2['left'] + 200)\n",
    "        pydirectinput.press('space')\n",
    "        return self.get_observation()\n",
    "\n",
    "    def render(self):\n",
    "        cv2.imshow('Game', np.array(self.cap.grab(self.game_location))[:,:,:3])\n",
    "        if cv2.waitKey(1) and 0xFF == ord('q'):\n",
    "            self.close()\n",
    "\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    def get_observation(self):        \n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3].astype(np.uint8)\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (self.crop_width, self.crop_height))\n",
    "        channel = np.reshape(resized, (1, self.crop_height, self.crop_width))\n",
    "        return channel\n",
    "\n",
    "        \n",
    "    def get_done(self):\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))[:,:,:3]\n",
    "        done_strings = [\"GAME\", \"GAHE\"]\n",
    "        done = False\n",
    "        res = pytesseract.image_to_string(done_cap)[:4]\n",
    "        if res in done_strings:\n",
    "            done = True\n",
    "        return done, done_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Display: {'left': 0, 'top': 0, 'width': 3926, 'height': 1440}\n",
      "Display 1:   {'left': 0, 'top': 0, 'width': 2560, 'height': 1440}\n",
      "Display 2:   {'left': 2560, 'top': 339, 'width': 1366, 'height': 768}\n"
     ]
    }
   ],
   "source": [
    "env = WebGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d691a8f760>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.get_observation()\n",
    "# plt.imshow(cv2.cvtColor(obs[0], cv2.COLOR_GRAY2BGR))\n",
    "plt.imshow(env.get_observation()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d691b9f580>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "status = env.get_done()\n",
    "print(status[0])\n",
    "plt.imshow(status[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './Saved_Models'\n",
    "LOG_DIR = './Logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=500, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Display: {'left': 0, 'top': 0, 'width': 3926, 'height': 1440}\n",
      "Display 1:   {'left': 0, 'top': 0, 'width': 2560, 'height': 1440}\n",
      "Display 2:   {'left': 2560, 'top': 339, 'width': 1366, 'height': 768}\n"
     ]
    }
   ],
   "source": [
    "env = WebGame()\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LatestModelLoad():\n",
    "    global model\n",
    "    path = \"C:\\\\AnCodeRR\\\\Google_Dino_Game_AI\\\\Saved_Models\\\\\"\n",
    "    saves = os.listdir(path=path)\n",
    "    if len(saves) == 0: \n",
    "        return\n",
    "    len_to_shave = len(\"best_model_\")\n",
    "    best_version = max([int(s[len_to_shave:-4]) for s in saves])\n",
    "    model = DQN.load(f\"C:\\\\AnCodeRR\\\\Google_Dino_Game_AI\\\\Saved_Models\\\\best_model_{best_version}.zip\", env=env, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LatestModelLoad()\n",
    "if not model:\n",
    "    model = DQN(\n",
    "        policy = 'CnnPolicy', \n",
    "        env = env, \n",
    "        tensorboard_log = LOG_DIR, \n",
    "        verbose = 1, \n",
    "        buffer_size = 100000, \n",
    "        learning_starts = 0)\n",
    "# model.save('./base.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./Logs/DQN_6\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.75     |\n",
      "|    ep_rew_mean      | 4.75     |\n",
      "|    exploration_rate | 0.819    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 19       |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.465    |\n",
      "|    n_updates        | 4        |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.25     |\n",
      "|    ep_rew_mean      | 6.25     |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 50       |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.375    |\n",
      "|    n_updates        | 12       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.33     |\n",
      "|    ep_rew_mean      | 6.33     |\n",
      "|    exploration_rate | 0.278    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 76       |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.237    |\n",
      "|    n_updates        | 18       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.44     |\n",
      "|    ep_rew_mean      | 6.44     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 103      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0633   |\n",
      "|    n_updates        | 25       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.4      |\n",
      "|    ep_rew_mean      | 6.4      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 128      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0358   |\n",
      "|    n_updates        | 31       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.33     |\n",
      "|    ep_rew_mean      | 6.33     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 152      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00637  |\n",
      "|    n_updates        | 37       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.29     |\n",
      "|    ep_rew_mean      | 6.29     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 176      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00743  |\n",
      "|    n_updates        | 43       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.72     |\n",
      "|    ep_rew_mean      | 6.72     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1        |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 215      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00327  |\n",
      "|    n_updates        | 53       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.53     |\n",
      "|    ep_rew_mean      | 7.53     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 271      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00106  |\n",
      "|    n_updates        | 67       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.22     |\n",
      "|    ep_rew_mean      | 8.22     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total_timesteps  | 329      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000628 |\n",
      "|    n_updates        | 82       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.09     |\n",
      "|    ep_rew_mean      | 8.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 2        |\n",
      "|    time_elapsed     | 154      |\n",
      "|    total_timesteps  | 356      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000892 |\n",
      "|    n_updates        | 88       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\AnCodeRR\\Google_Dino_Game_AI\\dino_game_ai.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:258\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    246\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    247\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    256\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[1;32m--> 258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(DQN, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    259\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    260\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    261\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    262\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    263\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    264\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    265\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    266\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    267\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    268\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    344\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    346\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 347\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[0;32m    348\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[0;32m    349\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[0;32m    350\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[0;32m    351\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    352\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[0;32m    353\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[0;32m    354\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    355\u001b[0m     )\n\u001b[0;32m    357\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:580\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    577\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    579\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m    582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    583\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "\u001b[1;32mc:\\AnCodeRR\\Google_Dino_Game_AI\\dino_game_ai.ipynb Cell 13\u001b[0m in \u001b[0;36mWebGame.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m action_map \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mspace\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mdown\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mno_op\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     pydirectinput\u001b[39m.\u001b[39;49mpress(action_map[action])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_done()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m new_observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_observation()\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\pydirectinput\\__init__.py:243\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[0;32m    242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39m_pause\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\pydirectinput\\__init__.py:232\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m _pause:\n\u001b[0;32m    231\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m     time\u001b[39m.\u001b[39;49msleep(PAUSE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_player(episodes, use_dqn=False):\n",
    "    i = 0\n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            i += 1 \n",
    "            if use_dqn:\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done, _ = env.step(int(action))\n",
    "            else:\n",
    "                obs, reward , done, _ = env.step(env.action_space.sample())\n",
    "            plt.imsave(fname=f\"C:\\AnCodeRR\\Google_Dino_Game_AI\\{i}.png\", arr=obs[0])\n",
    "            total_reward += reward\n",
    "        print(f'Total Reward for epoch {episode} is {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\AnCodeRR\\Google_Dino_Game_AI\\dino_game_ai.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m automated_player(\u001b[39m10\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\AnCodeRR\\Google_Dino_Game_AI\\dino_game_ai.ipynb Cell 16\u001b[0m in \u001b[0;36mautomated_player\u001b[1;34m(episodes, use_dqn)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     obs, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(\u001b[39mint\u001b[39m(action))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     obs, reward , done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49msample())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m plt\u001b[39m.\u001b[39mimsave(fname\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mAnCodeRR\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mGoogle_Dino_Game_AI\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m, arr\u001b[39m=\u001b[39mobs[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "\u001b[1;32mc:\\AnCodeRR\\Google_Dino_Game_AI\\dino_game_ai.ipynb Cell 16\u001b[0m in \u001b[0;36mWebGame.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m action_map \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mspace\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mdown\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mno_op\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     pydirectinput\u001b[39m.\u001b[39;49mpress(action_map[action])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_done()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/AnCodeRR/Google_Dino_Game_AI/dino_game_ai.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m new_observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_observation()\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\pydirectinput\\__init__.py:242\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m funcArgs \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetcallargs(wrappedFunction, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[1;32m--> 242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_pause\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\pydirectinput\\__init__.py:544\u001b[0m, in \u001b[0;36mpress\u001b[1;34m(keys, presses, interval, logScreenshot, _pause)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m keys:\n\u001b[0;32m    543\u001b[0m     failSafeCheck()\n\u001b[1;32m--> 544\u001b[0m     downed \u001b[39m=\u001b[39m keyDown(k)\n\u001b[0;32m    545\u001b[0m     upped \u001b[39m=\u001b[39m keyUp(k)\n\u001b[0;32m    546\u001b[0m     \u001b[39m# Count key press as complete if key was \"downed\" and \"upped\" successfully\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\pydirectinput\\__init__.py:243\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[0;32m    242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39m_pause\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Users\\deyol\\.conda\\envs\\rlPytorch\\lib\\site-packages\\pydirectinput\\__init__.py:232\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m _pause:\n\u001b[0;32m    231\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m     time\u001b[39m.\u001b[39;49msleep(PAUSE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# automated_player(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   automated_player(10, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rlPytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b3edbc63415a2456de106701fcc3627b7d1af623b890d6868dacd706e836efd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
